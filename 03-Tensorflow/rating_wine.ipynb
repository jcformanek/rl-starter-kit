{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Predicting the quality of wines\n",
    "Before we move on to implementing RL algorithms with neural networks I want to give you a brief overview of how we will be using Tensorflow. Tensorflow is a machine learning library. It lets us easily implement neural networks and perfrom operations such as stochastic gradient descent. \n",
    "\n",
    "I chose to use Tensorflow for my tutorials but another very popular alternative is PyTorch."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%%capture\n",
    "!pip install tensorflow\n",
    "!pip install tensorflow_datasets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensors\n",
    "In Tensorflow we perform mathematical operations on Tensors. Tensors are simply like higher dimensional arrays. We can for example make a 1-D tensor by wrapping a list in a Tensor. A 2-D tensor would be a matrix. We can then make 3-D, 4-D etc. tensors.\n",
    "\n",
    "Here is an example of how we construct tensors using Tensorflow. It is very easy to convert a numpy array into a tensor."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "my_list_of_lists = [[1,2,3], [4,5,6], [7,8,9]]\n",
    "my_np_array_of_arrays = np.array(my_list_of_lists)\n",
    "my_2d_tensor = tf.convert_to_tensor(my_np_array_of_arrays)\n",
    "\n",
    "print(\"Shape of Tensor:\", my_2d_tensor.shape)\n",
    "print(\"Tensor:\", my_2d_tensor)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-16 16:23:14.148279: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-08-16 16:23:14.148318: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of Tensor: (3, 3)\n",
      "Tensor: tf.Tensor(\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]], shape=(3, 3), dtype=int64)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-16 16:23:15.447393: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-08-16 16:23:15.447420: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-08-16 16:23:15.447442: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pop-os): /proc/driver/nvidia/version does not exist\n",
      "2021-08-16 16:23:15.447663: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we will make a 3-D tensor of shape `(100,8,8)`. This is a common way that batches of images are stored as a tensor. The first dimension is known as the batch dimension, in this case `100`. That means we have a batch of 100 images. The second and third dimensions are like the x and y coordinates of the pixels in the image.\n",
    "\n",
    "Below we make a tensor filled with zeros of shape `(100,8,8)` e.g. a batch of 100 images that have dimension `8x8`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "my_3d_tensor = tf.zeros(shape=(100,8,8))\n",
    "\n",
    "my_first_image = my_3d_tensor[0]\n",
    "print(\"First image shape:\", my_first_image.shape)\n",
    "\n",
    "first_image_first_row = my_3d_tensor[0,0]\n",
    "print(\"First row shape:\", first_image_first_row.shape)\n",
    "print(\"Row value:\", first_image_first_row)\n",
    "\n",
    "first_pixel = my_3d_tensor[0,0,0]\n",
    "print(\"First pixel shape:\", first_pixel.shape)\n",
    "print(\"Pixel value:\", first_pixel)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First image shape: (8, 8)\n",
      "First row shape: (8,)\n",
      "Row value: tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0.], shape=(8,), dtype=float32)\n",
      "First pixel shape: ()\n",
      "Pixel value: tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tensors are like numpy arrays but they have several other atributes that make them apropriate for machine learning. First of all, Tensorflow Tensors can be used on a GPU instead of just your computers CPU. Performing computations on a GPU can dramatically speed up your programs. If your PC does not have a GPU then you can use one for free on a Google Colab instance. A quick google search should help you figure out how to use a GPU on Google Colab.\n",
    "\n",
    "The second reason we use Tensorflow tensors is because they can keep track of the computations that were performed on them. This makes it possible to compute the gradients of mathematical operations performed on the tensors. This is especially useful for training neural networks because it is what makes it possible to perform stochastic gradient decent on the parameters of the neural network.\n",
    "\n",
    "In summary, whenever we want to pass values to our neural network we need to convert them into a tensor first."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Back to wine\n",
    "We are going to train a neural network to predict the rating of wines given a vector of information about the wines. The dataset we are going to use is available from Tensorflow itself. Below we will load the dataset. Visit the webpage to get information on what each value in the wines feature vector represents. [Webpage](https://www.tensorflow.org/datasets/catalog/wine_quality)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "## This cell is not important.\n",
    "# I just wrote a function to convert the data from the dataset into\n",
    "# something more usable. You don't really need to know how it works\n",
    "# but reading through it may behelpful because I demonstrate how to\n",
    "# reshape, cast and concatenate tensors.\n",
    "\n",
    "def preprocess_batch(batch):\n",
    "    feature_dict = batch[0]\n",
    "    labels = batch[1]\n",
    "\n",
    "    # Next we convert the dict of features\n",
    "    # into a list of tensors with the right \n",
    "    # dimensions.\n",
    "    list_of_tensors = []\n",
    "    for tensor in feature_dict.values():\n",
    "        # Reshape tensor (64,) -> (64, 1)\n",
    "        tensor = tf.reshape(tensor, shape=(-1, 1))\n",
    "        # Cast all tensors to dtype=float32\n",
    "        tensor = tf.cast(tensor, dtype='float32')\n",
    "        # Append tensor to the list\n",
    "        list_of_tensors.append(tensor)\n",
    "\n",
    "    # Concatenate list of tensors into one big tensor.\n",
    "    feature_tensor = tf.concat(list_of_tensors, axis=1)\n",
    "\n",
    "    # One hot encode labels\n",
    "    # i.e. the integer 2 becomes the vector [0,0,1,0,0,0,0,0,0,0]\n",
    "    on_hot_labels = tf.one_hot(indices=labels, depth=10)\n",
    "\n",
    "    # Return features and labels\n",
    "    return feature_tensor, on_hot_labels\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Download the dataset\n",
    "wine_dataset = tfds.load('wine_quality', split='train', shuffle_files=True, as_supervised=True)\n",
    "\n",
    "# Break the dataset up into batches of 64\n",
    "wine_dataset = wine_dataset.batch(64)\n",
    "\n",
    "# Loop through the dataset and\n",
    "# count  how many batches there are.\n",
    "cnt = 0\n",
    "for batch in wine_dataset:\n",
    "    # Lets print the shape of the first batch\n",
    "    # to see what the data looks like.\n",
    "    if cnt == 0:\n",
    "        # Preprocess\n",
    "        features, labels = preprocess_batch(batch)\n",
    "\n",
    "        # Lets print the features shape.\n",
    "        print(\"Feature shape:\", features.shape)\n",
    "\n",
    "        # Lets print one feature vector from the \n",
    "        # batch and its rating.\n",
    "        print(\"Wine feature vector:\", features[0])\n",
    "        print(\"Wine rating:\", labels[0])\n",
    "\n",
    "    # increment counter\n",
    "    cnt += 1\n",
    "\n",
    "print(\"Number of batches in the dataset:\", cnt)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Feature shape: (64, 11)\n",
      "Wine feature vector: tf.Tensor(\n",
      "[1.1200e+01 2.9000e-02 1.1000e-01 9.9076e-01 5.3000e+00 6.0000e+00\n",
      " 3.5100e+00 1.1000e+00 4.8000e-01 5.1000e+01 4.3000e-01], shape=(11,), dtype=float32)\n",
      "Wine rating: tf.Tensor([0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], shape=(10,), dtype=float32)\n",
      "Number of batches in the dataset: 77\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-16 16:23:16.022789: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-08-16 16:23:16.042630: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 1999965000 Hz\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, you should see from the above that our batch dimension is `64`. Our wine feature vector has length 11 and wines are rated on a scale from 1 to 10."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Networks\n",
    "Now lets create a simple neural network which accepts our feature vector as input and outputs a final layer with 10 nodes. We will then say that the node with the highest value on the final layer is the networks predicted rating for the wine, i.e. if the final layer looks like `[100, 2, 3, -40, 44, 22, 1, 6, 7, 9]` then because 100 is the largest number and is in the first position, the network predicts that the wine has a rating of `1`. \n",
    "\n",
    "Defining a feedforward neural network in Tensorflow is simple."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "hidden_layer = tf.keras.layers.Dense(units=100)\n",
    "activation = tf.keras.layers.ReLU()\n",
    "output_layer = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "list_of_layers = [hidden_layer, activation, output_layer]\n",
    "\n",
    "network = tf.keras.Sequential(layers=list_of_layers)\n",
    "\n",
    "# We can then send dummy data through the network \n",
    "# to initialize it. Since our feature vectors have \n",
    "# dimension 11 we can create the dummy input as follows.\n",
    "# We include a dummy batch dimension of 1.\n",
    "dummy_input = tf.ones(shape=(1,11), dtype='float32')\n",
    "\n",
    "dummy_output = network(dummy_input)\n",
    "print(\"Dummy output:\", dummy_output)\n",
    "\n",
    "print(network.summary())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dummy output: tf.Tensor(\n",
      "[[-0.00926837 -0.73155785  0.63351005 -0.17232516  0.04960261  0.18540995\n",
      "   0.14309533 -0.05527045 -0.43535423  0.65655017]], shape=(1, 10), dtype=float32)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (1, 100)                  1200      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (1, 100)                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, 10)                   1010      \n",
      "=================================================================\n",
      "Total params: 2,210\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now all we need to train our model is to define a loss function to measure how good the networks predictions are. Since this is a multi-class classification problem (with 10 classes) we can use the Categorical Crossentropy loss."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "loss_function = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Let the prediction be given by the neural network\n",
    "y_pred = dummy_output\n",
    "# Suppose the true label is 7\n",
    "# Dont worry about this confusing line. \n",
    "# It just computes the one-hot encoding of 7 with a batch dimension.\n",
    "y_true = tf.reshape(tf.one_hot(tf.convert_to_tensor(7), depth=10), shape=(1, -1))\n",
    "# We compute loss like this.\n",
    "loss = loss_function(y_true, y_pred)\n",
    "print(\"Loss:\", loss)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: tf.Tensor(2.4647036, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have a neural network to make predictions and a loss function to measure how good thos predictions are, all we need now is an optimizer that can perform gradient decent on the loss function and the parameters of the neural network to train our model. Lets quickly look at how we compute gradients in Tensordlow."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# All computations performed inside the scope \n",
    "# of this `with` statement will record their gradients \n",
    "# on the `tape`.\n",
    "with tf.GradientTape() as tape:\n",
    "    # Input.\n",
    "    dummy_input = tf.ones(shape=(1,11), dtype='float32')\n",
    "\n",
    "    # Pass through the network.\n",
    "    dummy_output = network(dummy_input)\n",
    "\n",
    "    # Dummy label 8.\n",
    "    dummy_label = tf.reshape(tf.one_hot(tf.convert_to_tensor(8), depth=10), shape=(1, -1))\n",
    "\n",
    "    # Compute the loss.\n",
    "    loss = loss_function(dummy_label, dummy_output)\n",
    "\n",
    "# Now that we are done with computing the loss \n",
    "# we can collect the trainable parameters in the neural network.\n",
    "trainable_variables = network.trainable_variables\n",
    "\n",
    "# We then retrieve the gradients from the `tape` by passing in the \n",
    "# loss and the variables with respect to which we want to compute \n",
    "# gradients with.\n",
    "gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "# We then create an optimizer.\n",
    "# In this case the Adam optimizer.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# We can then apply the gradients to the trainable parameters like this.\n",
    "gradients_and_variables = zip(gradients, trainable_variables)\n",
    "optimizer.apply_gradients(gradients_and_variables)\n",
    "\n",
    "print(\"Loss:\", loss)\n",
    "\n",
    "\n",
    "    \n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: tf.Tensor(2.8447871, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above computation was obviously nonesense but it should hopefully give you an idea of how we compute gradients and apply them to the neural network with an optimizer. Now that we have all of these moving parts, lets create a loop which takes a batch of feature vectors and labels, makes a prediction, computes the loss and applies a gradient update."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def learn_on_batch(network, loss_function, optimizer, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = network(features)\n",
    "\n",
    "        loss = loss_function(prediction, labels)\n",
    "\n",
    "        # Average the loss over the batch\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "    # Get gradients.\n",
    "    variables = network.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets iterate over the dataset and perform updates to the networks parameters using the above function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "for batch in wine_dataset:\n",
    "    # Preprocess\n",
    "    features, labels = preprocess_batch(batch)\n",
    "\n",
    "    loss = learn_on_batch(network, loss_function, optimizer, features, labels)\n",
    "\n",
    "    print(\"Batch loss:\", loss)\n",
    "\n",
    "print(\"Done.\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batch loss: tf.Tensor(-128.0379, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-136.45273, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-168.311, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-199.13275, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-227.26282, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-260.28647, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-265.4137, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-293.5562, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-337.6886, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-370.3993, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-373.4917, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-414.04315, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-474.88092, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-483.81873, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-512.786, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-525.64813, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-583.74915, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-571.06775, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-634.28033, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-620.4314, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-676.29486, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-713.67017, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-751.2527, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-801.823, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-795.5416, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-828.89703, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-945.5081, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-924.1972, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-967.8153, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-981.7302, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-1040.1237, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-1137.8918, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-1137.58, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-1136.4572, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-1211.8391, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-1252.723, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-1294.477, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-1299.4092, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-1331.3964, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-1411.9978, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-1479.9661, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-1476.416, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-1612.731, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-1518.5168, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-1694.6346, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-1871.5242, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-1694.4868, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-1766.7194, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-1734.0576, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-1852.5063, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-1950.4473, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-1971.8314, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-2101.089, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-1994.5067, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-2201.0938, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-2322.5664, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-2192.0469, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-2402.1038, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-2594.3562, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-2345.5642, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-2709.198, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-2682.7974, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-2553.9705, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-2587.508, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-2806.1572, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-2818.734, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-2890.6343, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-3008.755, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-3158.3835, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-3092.2544, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-3283.3408, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-3190.3315, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-3379.4307, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-3431.604, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-3462.138, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-3544.0176, shape=(), dtype=float32)\n",
      "Batch loss: tf.Tensor(-3637.0833, shape=(), dtype=float32)\n",
      "Done.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensorboard\n",
    "Another useful feature I want to share with you is Tensorboard. Tensorboard is a tool which you can use to log information to during training so that you can monitor things. Below is a class I define to log information to tensorboard."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import datetime\n",
    "\n",
    "class Logger():\n",
    "\n",
    "    def __init__(self, logdir=\"./logs/\"):\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        logdir = logdir + current_time\n",
    "\n",
    "        self.summary_writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "    def write(self, step, logs):\n",
    "        \"\"\"Write logs to tensorboard.\n",
    "\n",
    "        Args:\n",
    "            step (Int): Training step of the logs.\n",
    "            logs (Dict[str, float]): Dictionary of logs to be written to tensorboard.\n",
    "        \"\"\"\n",
    "        with self.summary_writer.as_default():\n",
    "            for key, value in logs.items():\n",
    "                tf.summary.scalar(key, value, step=step)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model training\n",
    "Lets pull together everything we have done so far. We will train a network on the wine dataset for 100 epochs. We will log the losses to tensorboard and monitor the process of training on tensorboard. We run the cell below to start tensorboard. If tensorboard does not apear, run the cell again."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 46409), started 0:14:51 ago. (Use '!kill 46409' to kill it.)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a0749738db4e3b46\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a0749738db4e3b46\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "logger = Logger()\n",
    "num_epochs = 100\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch in wine_dataset:\n",
    "        # Preprocess\n",
    "        features, labels = preprocess_batch(batch)\n",
    "\n",
    "        loss = learn_on_batch(network, loss_function, optimizer, features, labels)\n",
    "\n",
    "        epoch_losses.append(loss.numpy())\n",
    "    \n",
    "    epoch_avg_loss = np.mean(epoch_losses)\n",
    "\n",
    "    logger.write(e, {\"loss\": epoch_avg_loss})\n",
    "\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once you have run the cell above, go refresh tensorboard and monitor training. You will need to keep refreshin tensorboard to get the latest logs. You should hopefully see that the loss is going down, that suggests that our model is starting to get better at predicting the wine ratings. To verify that our model is really getting better we would need to evaluate it on a test dataset. But this tutorial is not really a lesson on supervised learning so I am gonna leave it here. Lets move on to RL now."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('rl-starter-kit': conda)"
  },
  "interpreter": {
   "hash": "2f1f09945eed0f0215de5d99819a5380b074734dbade509b2a9db3176055ac64"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}