{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('rl-starter-kit': conda)"
  },
  "interpreter": {
   "hash": "2f1f09945eed0f0215de5d99819a5380b074734dbade509b2a9db3176055ac64"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Agent-Environment Loop\n",
    "In RL we have an agent interacting with an environment. At every timestep $t$ the agent observes the state $s_t$ of the environment and choses an action $a_t$ to perform. Given the agent's action, the environment gives the agent a reward $r_t$ and transitions to a new state $s_{t+1}$. This interaction is known as the agent-environment interface and is given by the following diagram.\n",
    "\n",
    "![The Agent-Environment Interface](../images/agent-environment.png)\n",
    "\n",
    "We can implement this interface in code by building an Agent class and an Environment class. The Agent class should have a method `choose_action(state)` which, given a `state`, returns a valid `action`. The Environment should have a method `step(action)` which, given an `action`, should perform the given `action` in the environment and return the `next_state`.\n",
    "\n",
    "One way to represent the `state` of an environment is to use a feature vector. Each element in the vector represents a feature of the environment. As an example lets implement a simple grid-world environment where an agent needs to navigate from the bottom left of the grid-world to the top right. We can represent the `state` of the environment by a vector of the agent's x- and y-coordinates. In Python we can implement vectors using the `numpy` library."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# import the numpy library\n",
    "import numpy as np\n",
    "\n",
    "# We can convert lists in Python to numpy arrays.\n",
    "bottom_left = [0,0] # [x, y]\n",
    "print(\"List:\", bottom_left)\n",
    "bottom_left = np.array(bottom_left) # numpy array\n",
    "print(\"Numpy array:\", bottom_left)\n",
    "\n",
    "# Numpy arrays are powerful because they behave like vectors.\n",
    "# We can do vector addition and scalar multiplication using numpy arrays.\n",
    "move_right = np.array([1,0])\n",
    "print(\"Move right twice\", bottom_left + 2 * move_right) # move right twice"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "List: [0, 0]\n",
      "Numpy array: [0 0]\n",
      "Move right twice [2 0]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using numpy vectors we can now build our simple grid-world environment as a Python class with methods and attributes. We will need a couple methods which will be left for you to implement. There should be a `reset()` method which puts the agent back on the bottom left tile and sets the environment `done` flag to `False`. Next we need a `step(action)` method which given an action moves the agent to a new state in the environment. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class GridWorld():\n",
    "\n",
    "    # The initialiser function.\n",
    "    def __init__(self):\n",
    "        # Current internal state of the environment.\n",
    "        self.state = np.array([1,1])\n",
    "\n",
    "        # We can use a dictionary to store all the actions.\n",
    "        # So, actions[0] returns the numpy array [0 1].\n",
    "        # Similarly actions[3] returns [-1 0]\n",
    "        self.actions = {\n",
    "            0: np.array([0,1]), # move up\n",
    "            1: np.array([0,-1]), # move down\n",
    "            2: np.array([1,0]),  # move right\n",
    "            3: np.array([-1,0]) # move left\n",
    "        }\n",
    "\n",
    "        # Flag indicating if the environment is done. \n",
    "        self.done = False\n",
    "\n",
    "    # A private function to check if the given state\n",
    "    # is a terminal (death) state.\n",
    "    def _is_terminal_state(self, state):\n",
    "        if (state[0] <= 0 or state[0] >= 4 or\n",
    "            state[1] <= 0 or state[1] >= 4):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # A function to check if a state is the goal state.\n",
    "    def _is_goal_state(self, state):\n",
    "        if state[0] == 3 and state[1] == 3:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # A function to step the environment.\n",
    "    def step(self, action_id):\n",
    "        action = self.actions[action_id]\n",
    "\n",
    "        # Transition to next state.\n",
    "        next_state = self.state + action\n",
    "\n",
    "        # Check if action resulted in death.\n",
    "        if self._is_terminal_state(next_state):\n",
    "            self.done = True\n",
    "\n",
    "        # Check if next state is goal state.\n",
    "        if self._is_goal_state(next_state):\n",
    "            self.done = True\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        # Set environment internal state to next_state.\n",
    "        self.state = next_state\n",
    "\n",
    "        # Return tuple of (next_state, reward, done) for the agent.\n",
    "        return (self.state, reward, self.done)\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset done flag\n",
    "        self.done = False\n",
    "\n",
    "        # Reset agent position.\n",
    "        self.state = np.array([1,1])\n",
    "\n",
    "        return self.state\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we can create an agent that randomly chooses actions in the environment and hopes for the best. Our agent needs a function `choose_action(state)` that returns the id of the agents chosen action, in this case a random integer from the set {0,1,2,3}."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class RandomAgent():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        # Use numpy to chose a random action.\n",
    "        action_id = np.random.randint(4)\n",
    "\n",
    "        return action_id"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can then create the agent-environment loop as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def agent_environment_loop(agent, env):\n",
    "    #### String for prints ####\n",
    "    action_strings = {\n",
    "        0: \"up.\",\n",
    "        1: \"down.\", \n",
    "        2: \"right.\",\n",
    "        3: \"left.\"\n",
    "    }\n",
    "    ###########################\n",
    "    \n",
    "    # Reset the environment.\n",
    "    state = env.reset()\n",
    "    print(\"Agent's starting position:\", state)\n",
    "\n",
    "    while True:\n",
    "        # Agent chooses an action.\n",
    "        action = agent.choose_action(state)\n",
    "\n",
    "        # Print the agents action.\n",
    "        print(\"Agent moved\",action_strings[action])\n",
    "\n",
    "        # Step the environment.\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # Very important!!!\n",
    "        # Set state to next_state\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            if reward > 0:\n",
    "                print(\"Agent reached the goal!\")\n",
    "            else:\n",
    "                print(\"Agent died.\")\n",
    "                \n",
    "            # Exit the agent-environment loop.\n",
    "            break\n",
    "\n",
    "    print(\"Agent's final position:\", state)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Initalize agent and environment.\n",
    "agent = RandomAgent()\n",
    "env = GridWorld()\n",
    "\n",
    "agent_environment_loop(agent, env)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Agent's starting position: [1 1]\n",
      "Agent moved down.\n",
      "Agent died.\n",
      "Agent's final position: [1 0]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets make an agent that follows the optimal policy in this simple grid world."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class OptimalAgent():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if state[0] == 3:\n",
    "            action = 0 # move up\n",
    "        else:\n",
    "            action = 2 # move right\n",
    "\n",
    "        return action"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Initalize agent and environment.\n",
    "agent = OptimalAgent()\n",
    "env = GridWorld()\n",
    "\n",
    "agent_environment_loop(agent, env)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Agent's starting position: [1 1]\n",
      "Agent moved right.\n",
      "Agent moved right.\n",
      "Agent moved up.\n",
      "Agent moved up.\n",
      "Agent reached the goal!\n",
      "Agent's final position: [3 3]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}