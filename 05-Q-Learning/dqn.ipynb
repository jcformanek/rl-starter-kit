{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "!pip install tensorflow\n",
    "!pip install trfl\n",
    "!pip install gym"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting trfl\n",
      "  Using cached trfl-1.1.0-py3-none-any.whl (99 kB)\n",
      "Requirement already satisfied: numpy in /home/claude/.miniconda3/envs/rl-starter-kit/lib/python3.7/site-packages (from trfl) (1.19.5)\n",
      "Requirement already satisfied: wrapt in /home/claude/.miniconda3/envs/rl-starter-kit/lib/python3.7/site-packages (from trfl) (1.12.1)\n",
      "Requirement already satisfied: absl-py in /home/claude/.miniconda3/envs/rl-starter-kit/lib/python3.7/site-packages (from trfl) (0.13.0)\n",
      "Requirement already satisfied: dm-tree in /home/claude/.miniconda3/envs/rl-starter-kit/lib/python3.7/site-packages (from trfl) (0.1.6)\n",
      "Requirement already satisfied: six in /home/claude/.miniconda3/envs/rl-starter-kit/lib/python3.7/site-packages (from trfl) (1.15.0)\n",
      "Installing collected packages: trfl\n",
      "Successfully installed trfl-1.1.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import trfl\n",
    "\n",
    "class DQNAgent():\n",
    "\n",
    "    def __init__(self, act_dim, hidden_size=256, learning_rate=1e-3, gamma=0.99,\n",
    "        max_replay_size=10000, batch_size=256, epsilon_min=0.05, epsilon_dec=5e-6, \n",
    "        target_update=1000):\n",
    "        self.num_actions = act_dim\n",
    "        self.q_network = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Dense(hidden_size),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                tf.keras.layers.Dense(self.num_actions),\n",
    "            ]\n",
    "        )\n",
    "        self.target_q_network = copy.deepcopy(self.q_network)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        self.gamma = gamma\n",
    "        self.max_replay_size = max_replay_size\n",
    "        self.batch_size = batch_size\n",
    "        self.replay_buffer = []\n",
    "        self.replay_ctr = 0\n",
    "\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_dec = epsilon_dec\n",
    "\n",
    "        self.learn_ctr = 0\n",
    "        self.target_update = target_update\n",
    "\n",
    "    def decrement_epsilon(self):\n",
    "        # Decrement epsilon\n",
    "        self.epsilon -= self.epsilon_dec\n",
    "\n",
    "        # If less than epsilon min, set to epsilon min\n",
    "        if self.epsilon < self.epsilon_min:\n",
    "            self.epsilon = self.epsilon_min\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        # Epsilon greedy: Choose random action.\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        \n",
    "        # Epsilon greedy: Choose greedy action.\n",
    "        else:\n",
    "            # Convert numpy observation to tensor\n",
    "            observation = tf.convert_to_tensor(observation)\n",
    "\n",
    "            # Add batch dimension\n",
    "            observation = tf.expand_dims(observation, axis=0)\n",
    "\n",
    "            # Compute q-values\n",
    "            q_values = self.q_network(observation)\n",
    "\n",
    "            # Chose action with highest q-value\n",
    "            action = tf.argmax(q_values, axis=1)\n",
    "\n",
    "            # Convert tensor back to int\n",
    "            action = action.numpy()[0]\n",
    "\n",
    "        return action\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        if self.learn_ctr % self.target_update == 0:\n",
    "            self.target_q_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "    def store(self, observation, action, reward, next_observation, done):\n",
    "        experience_tuple = (observation, action, reward, next_observation, done)\n",
    "        \n",
    "        if self.replay_ctr < self.max_replay_size:\n",
    "            self.replay_buffer.append(experience_tuple)\n",
    "        else:\n",
    "            idx = self.replay_ctr % self.max_replay_size\n",
    "            self.replay_buffer[idx] = experience_tuple\n",
    "\n",
    "        self.replay_ctr += 1\n",
    "\n",
    "    def sample_replay(self):\n",
    "\n",
    "        # Get a random batch from replay\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        observations, actions, rewards, next_observations, dones = zip(*batch)\n",
    "\n",
    "        # Convert to tensors\n",
    "        observations = tf.convert_to_tensor(observations, dtype='float32')\n",
    "        actions = tf.convert_to_tensor(actions, dtype='int32')\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype='float32')\n",
    "        next_observations = tf.convert_to_tensor(next_observations, dtype='float32')\n",
    "        dones = tf.convert_to_tensor(dones, dtype='float32')\n",
    "\n",
    "        return observations, actions, rewards, next_observations, dones\n",
    "\n",
    "    def learn(self):\n",
    "        self.decrement_epsilon()\n",
    "\n",
    "        # If too litle data in replay, do nothing.\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        observations, actions, rewards, next_observations, dones = self.sample_replay()\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Compute q-value of observation and action pair.\n",
    "            q_values = self.q_network(observations)\n",
    "            # TODO explain what this does.\n",
    "            q_value = trfl.indexing_ops.batched_index(q_values, actions)\n",
    "\n",
    "            # Compute q-value of next observation using \n",
    "            # target network and `max` operator\n",
    "            next_q_value = tf.reduce_max(self.target_q_network(next_observations), axis=1)\n",
    "\n",
    "            # Bellman target\n",
    "            target = rewards + (1 - dones) * self.gamma * next_q_value\n",
    "\n",
    "            # Loss is just the mean square error\n",
    "            loss = tf.losses.MSE(target, q_value)\n",
    "\n",
    "        # Get trainable variables\n",
    "        variables = self.q_network.trainable_variables\n",
    "\n",
    "        # Compute gradients with respect to trainable variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "\n",
    "        # Apply gradients\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        # Maybe update the target network weights\n",
    "        self.update_target_network()\n",
    "\n",
    "        # Increment the learn counter\n",
    "        self.learn_ctr += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "source": [
    "def run_episode(environment, agent):\n",
    "    episode_return = 0\n",
    "    done = False\n",
    "    observation = environment.reset()\n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "\n",
    "        next_observation, reward, done, info = environment.step(action)\n",
    "\n",
    "        agent.store(observation, action, reward, next_observation, done)\n",
    "\n",
    "        observation = next_observation\n",
    "\n",
    "        episode_return += reward\n",
    "\n",
    "        # Agent learns at every step.\n",
    "        # Dont need to wait for end of episode.\n",
    "        agent.learn()\n",
    "    \n",
    "    return episode_return"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "source": [
    "import datetime\n",
    "\n",
    "class Logger():\n",
    "\n",
    "    def __init__(self, logdir=\"./logs/\"):\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        logdir = logdir + current_time\n",
    "\n",
    "        self.summary_writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "    def write(self, step, logs):\n",
    "        \"\"\"Write logs to tensorboard.\n",
    "\n",
    "        Args:\n",
    "            step (Int): Training step of the logs.\n",
    "            logs (Dict[str, float]): Dictionary of logs to be written to tensorboard.\n",
    "        \"\"\"\n",
    "        with self.summary_writer.as_default():\n",
    "            for key, value in logs.items():\n",
    "                tf.summary.scalar(key, value, step=step)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "source": [
    "def train(environment, agent, logger, num_iter=5000):\n",
    "    scores = []\n",
    "    for i in range(num_iter):\n",
    "        score = run_episode(environment, agent)\n",
    "        scores.append(score)\n",
    "\n",
    "        logs = {\n",
    "            \"return\": score,\n",
    "            \"epsilon\": agent.epsilon # Lets log epsilon too!\n",
    "        }\n",
    "        \n",
    "        logger.write(step=i, logs=logs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "source": [
    "import gym\n",
    "\n",
    "environment = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "act_dim = environment.action_space.n\n",
    "\n",
    "agent = DQNAgent(act_dim)\n",
    "\n",
    "episode_return = run_episode(environment, agent)\n",
    "print(\"Episode Return:\", episode_return)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode Return: -121.91977270430608\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "source": [
    "%load_ext tensorboard"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "source": [
    "%tensorboard --logdir logs/"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 45472), started 0:00:02 ago. (Use '!kill 45472' to kill it.)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-59138871c4e9e1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-59138871c4e9e1\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "source": [
    "logger = Logger()\n",
    "\n",
    "train(environment, agent, logger)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32683/3628842369.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_32683/2668896503.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(environment, agent, logger, num_iter)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_32683/2320698643.py\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(environment, agent)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Agent learns at every step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Dont need to wait for end of episode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mepisode_return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_32683/2186392471.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_observations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_32683/2186392471.py\u001b[0m in \u001b[0;36msample_replay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# Convert to tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mobservations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mnext_observations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_observations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/rl-starter-kit/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/rl-starter-kit/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1429\u001b[0m   \"\"\"\n\u001b[1;32m   1430\u001b[0m   return convert_to_tensor_v2(\n\u001b[0;32m-> 1431\u001b[0;31m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0m\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/rl-starter-kit/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1439\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/rl-starter-kit/lib/python3.7/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/rl-starter-kit/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/rl-starter-kit/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_autopacking_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m   \u001b[0;34m\"\"\"Tensor conversion function that automatically packs arguments.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mas_ref\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_should_not_autopack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1526\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m   \u001b[0minferred_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_dtype_from_nested_lists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/rl-starter-kit/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_should_not_autopack\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m   1517\u001b[0m   \u001b[0;31m# pylint: disable=unidiomatic-typecheck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1518\u001b[0m   \u001b[0;31m# TODO(slebedev): add nest.all?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1519\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_NON_AUTOPACKABLE_TYPES\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1520\u001b[0m   \u001b[0;31m# pylint: enable=unidiomatic-typecheck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/rl-starter-kit/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mflatten\u001b[0;34m(structure, expand_composites)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m   \u001b[0mexpand_composites\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('rl-starter-kit': conda)"
  },
  "interpreter": {
   "hash": "2f1f09945eed0f0215de5d99819a5380b074734dbade509b2a9db3176055ac64"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}