{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DQN\n",
    "Now that we know how to use neural networks in Tensorflow, lets implement our first RL algorithm to solve an RL environment. \n",
    "\n",
    "## LunarLander\n",
    "The environment we will use is the OpenAI Lunar Lander environment. In this environment the agent must safely land a spaceship on the moon by carefully controlling the spaceship's thrusters. Below is a screenshot of the environment. Lets quickly evaluate a random agent on the environment to see how well it does.\n",
    "\n",
    "![Lunar Lander](../images/lunarlander.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "%%capture\n",
    "# The line above hides the output of this cell.\n",
    "# I do this for readability.\n",
    "\n",
    "# Install dependencies\n",
    "!pip install tensorflow\n",
    "!pip install trfl\n",
    "!pip install gym"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "environment = gym.make(\"LunarLander-v2\")\n",
    "returns = []\n",
    "for i in range(100):\n",
    "    episode_return = 0\n",
    "    observation = environment.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = environment.action_space.sample()\n",
    "\n",
    "        next_observation, reward, done, info = environment.step(action)\n",
    "\n",
    "        episode_return += reward\n",
    "\n",
    "        observation = next_observation\n",
    "\n",
    "    returns.append(episode_return)\n",
    "\n",
    "print(\"Average Return for Random Agent:\", np.average(returns))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average Return for Random Agent: -201.8759844761783\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, a random agent on Lunar Lander gets a score of around -200. A winning score is +200. To create a winning agent we will implement DQN.\n",
    "\n",
    "## DQN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "import tensorflow as tf\n",
    "import trfl\n",
    "\n",
    "class DQNAgent():\n",
    "\n",
    "    def __init__(self, act_dim, hidden_size=256, learning_rate=1e-3, gamma=0.99,\n",
    "        max_replay_size=10000, batch_size=256, epsilon_min=0.05, epsilon_dec=5e-6, \n",
    "        target_update=1000):\n",
    "        self.num_actions = act_dim\n",
    "\n",
    "        # A simple feed forward neural network\n",
    "        # as our q-network\n",
    "        self.q_network = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Dense(hidden_size),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                tf.keras.layers.Dense(self.num_actions),\n",
    "            ]\n",
    "        )\n",
    "        # The target q-network is just a copy of the online network\n",
    "        self.target_q_network = copy.deepcopy(self.q_network)\n",
    "        # The network optimizer\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        # Discount\n",
    "        self.gamma = gamma\n",
    "        # Super basic replay buffer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_replay_size = max_replay_size\n",
    "        self.replay_buffer = []\n",
    "        self.replay_ctr = 0\n",
    "        # Exploration\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_dec = epsilon_dec\n",
    "        # Variables for periodically updating target\n",
    "        self.learn_ctr = 0\n",
    "        self.target_update = target_update\n",
    "\n",
    "    def decrement_epsilon(self):\n",
    "        # Decrement epsilon\n",
    "        self.epsilon -= self.epsilon_dec # linear decay\n",
    "        # You could experiment with exponential decay\n",
    "\n",
    "        # If less than epsilon min, set to epsilon min\n",
    "        if self.epsilon < self.epsilon_min:\n",
    "            self.epsilon = self.epsilon_min\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        # Epsilon greedy: Choose random action.\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        \n",
    "        # Epsilon greedy: Choose greedy action.\n",
    "        else:\n",
    "            # Convert numpy observation to tensor\n",
    "            observation = tf.convert_to_tensor(observation)\n",
    "\n",
    "            # Add batch dimension\n",
    "            observation = tf.expand_dims(observation, axis=0)\n",
    "\n",
    "            # Compute q-values\n",
    "            q_values = self.q_network(observation)\n",
    "\n",
    "            # Chose action with highest q-value\n",
    "            action = tf.argmax(q_values, axis=1)\n",
    "\n",
    "            # Convert tensor back to int\n",
    "            action = action.numpy()[0]\n",
    "\n",
    "        return action\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        # Periodicaly update the target network to be \n",
    "        # equal to the online network.\n",
    "        # If update is too frequent, learning will be unstable.\n",
    "        # If update is not frequent enough, learning will be slow.\n",
    "        if self.learn_ctr % self.target_update == 0:\n",
    "            self.target_q_network.set_weights(self.q_network.get_weights()) # hard update\n",
    "            # You can experiment with a soft update (polyak averaging)\n",
    "\n",
    "\n",
    "    def store(self, observation, action, reward, next_observation, done):\n",
    "        # This method stores experience tuples in replay.\n",
    "        experience_tuple = (observation, action, reward, next_observation, done)\n",
    "        \n",
    "        if self.replay_ctr < self.max_replay_size:\n",
    "            self.replay_buffer.append(experience_tuple)\n",
    "        else:\n",
    "            # If replay is full, replace the oldest sample \n",
    "            # with new sample.\n",
    "            idx = self.replay_ctr % self.max_replay_size\n",
    "            self.replay_buffer[idx] = experience_tuple\n",
    "\n",
    "        # Increment the replay counter.\n",
    "        self.replay_ctr += 1\n",
    "\n",
    "    def sample_replay(self):\n",
    "\n",
    "        # Get a random batch from replay\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size) # super slow, make better\n",
    "\n",
    "        # Convert list of tuples into tuple of lists.\n",
    "        observations, actions, rewards, next_observations, dones = zip(*batch)\n",
    "\n",
    "        # Convert to tensors\n",
    "        observations = tf.convert_to_tensor(observations, dtype='float32')\n",
    "        actions = tf.convert_to_tensor(actions, dtype='int32')\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype='float32')\n",
    "        next_observations = tf.convert_to_tensor(next_observations, dtype='float32')\n",
    "        dones = tf.convert_to_tensor(dones, dtype='float32')\n",
    "\n",
    "        return observations, actions, rewards, next_observations, dones\n",
    "\n",
    "    def learn(self):\n",
    "        self.decrement_epsilon()\n",
    "\n",
    "        # If too litle data in replay, do nothing.\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Sample from replay\n",
    "        observations, actions, rewards, next_observations, dones = self.sample_replay()\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Compute q-value of observation and action pair.\n",
    "            q_values = self.q_network(observations)\n",
    "\n",
    "            # We now need to select the q-value of the action the agent took.\n",
    "            # `trfl` is a python package with useful, reusable RL methods. Check it out!! \n",
    "            # This method takes a tensor of q-values (shape=[B, num_actions]) and a \n",
    "            # tensor of actions (shape=[B]) and returns the q-values indexed by action\n",
    "            # shape=[B], e.g. if q-values is [[2, 3, 5], [2, 6, 7]] and agent actions\n",
    "            # are [2, 1] then the method returns [5, 6].\n",
    "            q_value = trfl.indexing_ops.batched_index(q_values, actions)\n",
    "\n",
    "            # Compute q-value of next observation using \n",
    "            # target network and `max` operator\n",
    "            next_q_value = tf.reduce_max(self.target_q_network(next_observations), axis=1)\n",
    "\n",
    "            # Bellman target\n",
    "            target = rewards + (1 - dones) * self.gamma * next_q_value\n",
    "\n",
    "            # Loss is just the mean square error\n",
    "            # between the predicted q-value and the target.\n",
    "            loss = tf.losses.MSE(target, q_value)\n",
    "\n",
    "        # Get trainable variables\n",
    "        variables = self.q_network.trainable_variables\n",
    "\n",
    "        # Compute gradients with respect to trainable variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "\n",
    "        # Apply gradients\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        # Maybe update the target network weights\n",
    "        self.update_target_network()\n",
    "\n",
    "        # Increment the learn counter\n",
    "        self.learn_ctr += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import datetime\n",
    "\n",
    "# We define a class to log training progress to tensorboard.\n",
    "class Logger():\n",
    "    \"\"\"\n",
    "    An object to log data to tensorboard.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logdir=\"./logs/\"):\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        logdir = logdir + current_time\n",
    "\n",
    "        self.summary_writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "    def write(self, step, logs):\n",
    "        \"\"\"Write logs to tensorboard.\n",
    "\n",
    "        Args:\n",
    "            step (Int): Training step of the logs.\n",
    "            logs (Dict[str, float]): Dictionary of logs to be written to tensorboard.\n",
    "        \"\"\"\n",
    "        with self.summary_writer.as_default():\n",
    "            for key, value in logs.items():\n",
    "                tf.summary.scalar(key, value, step=step)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def run_episode(environment, agent):\n",
    "    \"\"\"A function to run one episode.\n",
    "\n",
    "    Args:\n",
    "        environment (gym.environment): The gym environment to train the agent on.\n",
    "        agent: The agent interacting with the environment.\n",
    "\n",
    "    Returns:\n",
    "        [float]: The return of the episode.\n",
    "    \"\"\"\n",
    "    episode_return = 0\n",
    "    done = False\n",
    "    observation = environment.reset()\n",
    "    while not done:\n",
    "        # Choose action\n",
    "        action = agent.choose_action(observation)\n",
    "\n",
    "        # Step env\n",
    "        next_observation, reward, done, info = environment.step(action)\n",
    "\n",
    "        # Store experience\n",
    "        agent.store(observation, action, reward, next_observation, done)\n",
    "\n",
    "        # Update observation. Critical!!\n",
    "        observation = next_observation\n",
    "\n",
    "        # Add reward to episode return for logging.\n",
    "        episode_return += reward\n",
    "\n",
    "        # Agent learns at every step.\n",
    "        # Dont need to wait for end of episode.\n",
    "        agent.learn()\n",
    "    \n",
    "    return episode_return"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def train(environment, agent, logger, num_iter=5000):\n",
    "    \"\"\"Funtion to train an agent in the environment.\n",
    "\n",
    "    Args:\n",
    "        environment: The environment.\n",
    "        agent: The agent.\n",
    "        logger: A tensorboard logger.\n",
    "        num_iter (int, optional): Number of training episodes. Defaults to 5000.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for i in range(num_iter):\n",
    "        score = run_episode(environment, agent)\n",
    "        scores.append(score)\n",
    "\n",
    "        # Stuff to write to tensorboard.\n",
    "        logs = {\n",
    "            \"return\": score,\n",
    "            \"epsilon\": agent.epsilon # Lets log epsilon too!\n",
    "        }\n",
    "        # Write to tensorboard\n",
    "        logger.write(step=i, logs=logs)\n",
    "\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "%%capture\n",
    "import gym\n",
    "\n",
    "# Instantiate environment, agent and logger\n",
    "environment = gym.make(\"LunarLander-v2\")\n",
    "act_dim = environment.action_space.n\n",
    "agent = DQNAgent(act_dim)\n",
    "logger = Logger()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# Run this cell if you want to monitor training with tensorboard.\n",
    "# If tensorboard does not appear, run the cell a second time.\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 121172), started 0:00:02 ago. (Use '!kill 121172' to kill it.)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-dc43d405174519c3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-dc43d405174519c3\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# Run this cell to start training.\n",
    "returns = train(environment, agent, logger, num_iter=1000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Results\n",
    "\n",
    "Here is a screen shot of the results after 1000 training episodes.\n",
    "![DQN Results](../images/dqn_results.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('rl-starter-kit': conda)"
  },
  "interpreter": {
   "hash": "2f1f09945eed0f0215de5d99819a5380b074734dbade509b2a9db3176055ac64"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}