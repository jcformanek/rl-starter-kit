{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# REINFORCE"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%%capture\n",
    "!pip install gym\n",
    "!pip install box2d.py\n",
    "!pip install tensorflow\n",
    "!pip install tensorflow_probability"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute the reward to go for an episode.\n",
    "def reward_to_go(rewards, gamma):\n",
    "    rewards = np.array(rewards)\n",
    "    rewtg = np.zeros_like(rewards, dtype='float32')\n",
    "    T = len(rewards) - 1\n",
    "    rewtg[T] = rewards[T]\n",
    "    for t in range(T-1, -1, -1):\n",
    "        rewtg[t] = rewards[t] + gamma * rewtg[t+1]\n",
    "    return rewtg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "class REINFORCEAgent():\n",
    "\n",
    "    def __init__(self, act_dim, hidden_size=100, learning_rate=1e-3, gamma=0.9):\n",
    "        self.policy_network = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Dense(hidden_size),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                tf.keras.layers.Dense(act_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        self.gamma = gamma\n",
    "        self.replay_buffer = []\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        observation = tf.convert_to_tensor(observation)\n",
    "        observation = tf.expand_dims(observation, axis=0)\n",
    "        logits = self.policy_network(observation)\n",
    "        policy = tfp.distributions.Categorical(logits=logits)\n",
    "        action = policy.sample()\n",
    "        return action.numpy()[0]\n",
    "        \n",
    "\n",
    "    def store(self, observation, action, reward, next_observation):\n",
    "        experience_tuple = (observation, action, reward, next_observation)\n",
    "        self.replay_buffer.append(experience_tuple)\n",
    "\n",
    "    def learn(self):\n",
    "        observations, actions, rewards, next_observations = zip(*self.replay_buffer)\n",
    "\n",
    "        observations = tf.convert_to_tensor(observations)\n",
    "        actions = tf.convert_to_tensor(actions)\n",
    "\n",
    "        rewtg = reward_to_go(rewards, self.gamma)\n",
    "        rewtg = tf.convert_to_tensor(rewtg)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.policy_network(observations)\n",
    "            policy = tfp.distributions.Categorical(logits=logits)\n",
    "            log_probs = policy.log_prob(actions)\n",
    "\n",
    "            loss = -tf.reduce_sum(log_probs * rewtg)\n",
    "\n",
    "        variables = self.policy_network.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        self.replay_buffer = []\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-02 15:43:42.023670: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-08-02 15:43:42.023702: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def run_episode(environment, agent):\n",
    "    episode_return = 0\n",
    "    done = False\n",
    "    observation = environment.reset()\n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "\n",
    "        next_observation, reward, done, info = environment.step(action)\n",
    "\n",
    "        agent.store(observation, action, reward, next_observation)\n",
    "\n",
    "        observation = next_observation\n",
    "\n",
    "        episode_return += reward\n",
    "    \n",
    "    return episode_return\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import gym\n",
    "\n",
    "environment = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "act_dim = environment.action_space.n\n",
    "\n",
    "agent = REINFORCEAgent(act_dim)\n",
    "\n",
    "episode_return = run_episode(environment, agent)\n",
    "print(\"Episode Return:\", episode_return)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-02 15:43:52.937823: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-08-02 15:43:52.937855: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-08-02 15:43:52.937877: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pop-os): /proc/driver/nvidia/version does not exist\n",
      "2021-08-02 15:43:52.938185: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode Return: -507.0328812567266\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import datetime\n",
    "\n",
    "class Logger():\n",
    "\n",
    "    def __init__(self, logdir=\"./logs/\"):\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        logdir = logdir + current_time\n",
    "\n",
    "        self.summary_writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "    def write(self, step, logs):\n",
    "        \"\"\"Write logs to tensorboard.\n",
    "\n",
    "        Args:\n",
    "            step (Int): Training step of the logs.\n",
    "            logs (Dict[str, float]): Dictionary of logs to be written to tensorboard.\n",
    "        \"\"\"\n",
    "        with self.summary_writer.as_default():\n",
    "            for key, value in logs.items():\n",
    "                tf.summary.scalar(key, value, step=step)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def train(environment, agent, logger, num_iter=5000):\n",
    "    scores = []\n",
    "    for i in range(num_iter):\n",
    "        score = run_episode(environment, agent)\n",
    "        agent.learn()\n",
    "        scores.append(score)\n",
    "\n",
    "        logger.write(step=i, logs={\"return\": score})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "%load_ext tensorboard"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 30024), started 0:00:18 ago. (Use '!kill 30024' to kill it.)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9b79faa887da4aba\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9b79faa887da4aba\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%tensorboard --logdir logs/"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "logger = Logger()\n",
    "\n",
    "train(environment, agent, logger)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('rl-starter-kit': conda)"
  },
  "interpreter": {
   "hash": "2f1f09945eed0f0215de5d99819a5380b074734dbade509b2a9db3176055ac64"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}