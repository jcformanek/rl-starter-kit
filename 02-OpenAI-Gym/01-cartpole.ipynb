{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('rl-starter-kit': conda)"
  },
  "interpreter": {
   "hash": "2f1f09945eed0f0215de5d99819a5380b074734dbade509b2a9db3176055ac64"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# OpenAI Gym\n",
    "The research company OpenAI released a super useful python package for RL called Gym. OpenAI's Gym is a collection of environments or games which researchers can use to train RL agents with. All of the environments adhere to  the same simple API which is not unlike the one I described grid-world notebook. In this notebook I will walk you through an example of how to use the OpenAI Gym. \n",
    "\n",
    "## Installing the gym python package\n",
    "We install gym by running the following cell."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Install OpenAi Gym\n",
    "!pip install gym"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: gym in /home/claude/.miniconda3/envs/rl-starter-kit/lib/python3.7/site-packages (0.18.3)\n",
      "Requirement already satisfied: scipy in /home/claude/.miniconda3/envs/rl-starter-kit/lib/python3.7/site-packages (from gym) (1.7.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /home/claude/.miniconda3/envs/rl-starter-kit/lib/python3.7/site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/claude/.miniconda3/envs/rl-starter-kit/lib/python3.7/site-packages (from gym) (1.21.1)\n",
      "Requirement already satisfied: pyglet<=1.5.15,>=1.4.0 in /home/claude/.miniconda3/envs/rl-starter-kit/lib/python3.7/site-packages (from gym) (1.5.15)\n",
      "Requirement already satisfied: Pillow<=8.2.0 in /home/claude/.miniconda3/envs/rl-starter-kit/lib/python3.7/site-packages (from gym) (8.2.0)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CartPole\n",
    "The environment we will consider first is a simple game called CartPole. In some sense, CartPole is like the \"HelloWorld\" of RL, it is a right of passage for all RL researchers. \n",
    "\n",
    "The challenge is for the agent to balance a pole on a cart by moving the cart left and right. Below is an image of the environment setup. \n",
    "\n",
    "![CartPole](../images/cartpole.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### State and Action Space\n",
    "In CartPole the agent can choose from two discreet actions: \"move left\" and \"move right\". The state of the environment is given by a vector with four values which represent the \"horisontal position\" and \"horisontal velocity\" of the cart, and \"angular position\" and \"angular velocity\" of the pole. It is clear that an agent with access to these 4 values should be able to optimally control the cart.\n",
    "\n",
    "Lets instantiate the environment and look at the shapes of the states and actions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import gym\n",
    "\n",
    "# Instantiate the environment\n",
    "environment = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(\"State_space:\", environment.observation_space)\n",
    "print(\"Action Space:\", environment.action_space)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "State_space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "Action Space: Discrete(2)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A `Box` is an internal Gym object which is used to describe a vector with bounded values. The first two numbers in the `Box` above indicate the maximum and minimum values the entries in the vector can take on. The third entry in the `Box` is the shape of the vector, in this case the state vector has length 4. Finally, the final entry in the `Box` gives the data type in the state vector, in this case simply a floating point number. \n",
    "\n",
    "`Discreet` is another Gym object used to discribe a discreet action space. In this case the environment has two discreet actions, usally actions are given by integers starting at zero. So in this case the agent has action `0` and action `1`.\n",
    "\n",
    "### Reset() and Sampling Random Actions\n",
    "Lets now look at an example state vector and an example action. All Gym environments have a `reset()` method which resets the environment and returns the first observation. Remember to call this when ever your agent starts a new episode in the environment. Every environment also has a method to sample a valid random action from the action space."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "observation = environment.reset()\n",
    "print(\"Initial Observation:\", observation)\n",
    "\n",
    "action = environment.action_space.sample()\n",
    "print(\"Random action:\", action)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial Observation: [ 0.01980218 -0.01997417  0.0087345  -0.01244694]\n",
      "Random action: 1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Step() Function\n",
    "Gym environments has a `step()` method which takes an action as input and returns the resulting next state of the environment, as well as the agents reward and done flag. The done flag is a boolean which indicates if the agent has entered a terminal state or not. The step function alsoreturns some extra info, but we usually don't need to worry about these. Here is an example of how we can step() the environment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Sample a valid random action\n",
    "action = environment.action_space.sample()\n",
    "\n",
    "# Step the environment\n",
    "next_observation, reward, done, info = environment.step(action)\n",
    "\n",
    "print(\"Next Observation:\", observation)\n",
    "print(\"Reward:\", reward)\n",
    "print(\"Done:\", done)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Next Observation: [ 0.01980218 -0.01997417  0.0087345  -0.01244694]\n",
      "Reward: 1.0\n",
      "Done: False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reward Function\n",
    "Every environment in Gym has a different reward pattern. In CartPole it is very simple, at every timestep where the agent is alive it receives +1 reward. If the agent dies it gets a reward of 0. Thus the reward signal incentivises the agent to keep the pole balancing for as long as possible.\n",
    "\n",
    "### The Agent-Environment Loop\n",
    "We now have all of the components we need to make a random agent randomly choose actions in the environment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Always reset the environment at the start of an episode\n",
    "observation = environment.reset()\n",
    "\n",
    "# Store the sum of rewards in a variable called `episode_return`\n",
    "episode_return = 0\n",
    "\n",
    "# Initially set the `done` flag to False\n",
    "done = False\n",
    "\n",
    "# Loop until 'done' == True\n",
    "while not done:\n",
    "    # Agent chooses action\n",
    "    # In this case we chose sample a random action\n",
    "    action = environment.action_space.sample()\n",
    "\n",
    "    # Step the environment\n",
    "    next_observation, reward, done, info = environment.step(action)\n",
    "\n",
    "    # Add the reward to `episode_return`\n",
    "    episode_return += reward\n",
    "\n",
    "    # Critical!!!\n",
    "    # Set current observation to next observation\n",
    "    observation = next_observation\n",
    "\n",
    "print(\"Episode Return:\", episode_return)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode Return: 18.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You should try running the cell above a couple of times and see how variable the agents performance can be. You should find the random agent usually gets a episode return of around 20-40. The CartPole environment is considered solved if the agent can consistently get an epsiode return of 500.\n",
    "\n",
    "To get an agent to solve the environment we could use RL. But it turns out that CartPole is such a simple environment that a simple monticarlo algorithm can solve it in a few hundred steps. So as an exercise lets solve CartPole in the simplest way I know how. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Agent\n",
    "Our agent is going to have an internal 4-vector of weights which we will use to compute the dot product with the state observation vector. If the result of the dot product is greater than zero the agent will chose to go right. If the result is less than zero the agent will chose to go left. \n",
    "\n",
    "We will randomly chose a set of weights and then evaluate it in the environment. Whenever we find a set of weights that are better than the current best set of weights, we will store them. Lets see if our simple agent can find a set of weights that solve the environment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# We will need numpy for this\n",
    "import numpy as np\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.weights = self.new_random_weights()\n",
    "\n",
    "    def new_random_weights(self):\n",
    "        # Center weights around zero\n",
    "        # min value of -1 and max value +1\n",
    "        self.weights = -1 + 2 * np.random.rand(4)\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        dot = np.matmul(self.weights, observation)\n",
    "\n",
    "        action = 0 if dot < 0 else 1\n",
    "\n",
    "        return action"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Montecarlo Algorithm\n",
    "We intantiate the agent and let it interact with the environment. We store the best weights.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# We will need copy\n",
    "import copy\n",
    "\n",
    "# Agent\n",
    "agent = Agent()\n",
    "\n",
    "# Variable to keep track of best score and weights\n",
    "best_score = 0\n",
    "best_weights = None\n",
    "\n",
    "# Maximum number of random trials\n",
    "max_num_trials = 100000\n",
    "for i in range(max_num_trials):\n",
    "    \n",
    "    # Get new weights for agent\n",
    "    agent.new_random_weights()\n",
    "\n",
    "    # Reset the environment\n",
    "    observation = environment.reset()\n",
    "\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        # Agent chooses action\n",
    "        action = agent.choose_action(observation)\n",
    "\n",
    "        # Step environment\n",
    "        next_observation, reward, done, info = environment.step(action)\n",
    "\n",
    "        #Add reward to score\n",
    "        score += reward\n",
    "\n",
    "        # Critical!!!\n",
    "        observation = next_observation\n",
    "\n",
    "    # Check if score is new high score\n",
    "    if score > best_score:\n",
    "        # Store a copy of the agents weights\n",
    "        best_weights = copy.deepcopy(agent.weights)\n",
    "        # Store best score\n",
    "        best_score = score\n",
    "        # Print new best score\n",
    "        print(\"New best score:\", best_score)\n",
    "\n",
    "    # Break out of loop if we found the optimal weights\n",
    "    if best_score >= 500:\n",
    "        print(f\"Optimal weights found in {i} steps!!\")\n",
    "        break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "New best score: 9.0\n",
      "New best score: 500.0\n",
      "Optimal weights found in 2 steps!!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets verify that the weights we found do solve the environment. We will let the agent interact with the environment 100 times and see what its average score is over all 100 runs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Load best weights into the agent\n",
    "agent.weights = best_weights\n",
    "\n",
    "scores = []\n",
    "for i in range(100):\n",
    "    done = False\n",
    "    score = 0\n",
    "    observation = environment.reset()\n",
    "    while not done:\n",
    "        # Agent chooses action\n",
    "        action = agent.choose_action(observation)\n",
    "\n",
    "        # Step environment\n",
    "        next_observation, reward, done, info = environment.step(action)\n",
    "\n",
    "        #Add reward to score\n",
    "        score += reward\n",
    "\n",
    "        # Critical!!!\n",
    "        observation = next_observation\n",
    "\n",
    "    scores.append(score)\n",
    "\n",
    "print(\"Average Episode Return:\", np.average(scores))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average Episode Return: 428.22\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, our agent is near optimal. So, CartPole is not the most difficult game to master."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ]
}